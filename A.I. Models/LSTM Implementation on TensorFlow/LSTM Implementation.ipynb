{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long Short Term Memory for Time Series based Forecasting Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 7\n",
    "WINDOW_SIZE = 7\n",
    "HIDDEN_UNITS = 256\n",
    "FEATURES = 1\n",
    "GRAD_CLIP = 7 #Clip gradients at this value if they go over it.\n",
    "LR = 0.001\n",
    "EPOCHS = 700\n",
    "NUM_FUT_PRED = 2 #No. of future time steps to predict\n",
    "#TARGET_FEATURE_INDEX = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since inputs to the LSTM Network is of shape [observations, time steps, features], therefore we store these using $\\verb|tf.placeholder|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.placeholder(tf.float32,(BATCH_SIZE,WINDOW_SIZE,FEATURES))\n",
    "targets = tf.placeholder(tf.float32,(BATCH_SIZE,NUM_FUT_PRED))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The governing equations for the LSTM Cell are:\n",
    "\n",
    "$c_t = f\\circ c_{t-1} + i \\circ g$   \n",
    "\n",
    "$h_t = o \\circ tanh(c_t)$\n",
    "\n",
    "where, $c_t$ is the hidden cell state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. INPUT GATE\n",
    "\n",
    "$i = \\sigma(W*[h_{t-1},x_t] + b_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_input_gate = tf.Variable(tf.truncated_normal([FEATURES,HIDDEN_UNITS],stddev = 0.001))\n",
    "weights_input_hidden = tf.Variable(tf.truncated_normal([HIDDEN_UNITS,HIDDEN_UNITS],stddev = 0.001))\n",
    "bias_input = tf.Variable(tf.zeros([HIDDEN_UNITS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. FORGET GATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$f = \\sigma (W*[h_{t-1},x_t] + b_f)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_forget_gate = tf.Variable(tf.truncated_normal([FEATURES,HIDDEN_UNITS],stddev = 0.001))\n",
    "weights_forget_hidden = tf.Variable(tf.truncated_normal([HIDDEN_UNITS,HIDDEN_UNITS],stddev = 0.001))\n",
    "bias_forget = tf.Variable(tf.zeros([HIDDEN_UNITS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. OUTPUT GATE\n",
    "\n",
    "$o = \\sigma(W*[h_{t-1},x_t] + b_o)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_output_gate = tf.Variable(tf.truncated_normal([FEATURES,HIDDEN_UNITS],stddev = 0.001))\n",
    "weights_output_hidden = tf.Variable(tf.truncated_normal([HIDDEN_UNITS,HIDDEN_UNITS],stddev = 0.001))\n",
    "bias_output = tf.Variable(tf.zeros([HIDDEN_UNITS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GATE GATE (Also, Memory Gate)\n",
    "\n",
    "$g = \\tanh(W*[h_{t-1},x_t] + b_g)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_gate_gate = tf.Variable(tf.truncated_normal([FEATURES,HIDDEN_UNITS],stddev = 0.001))\n",
    "weights_gate_hidden = tf.Variable(tf.truncated_normal([HIDDEN_UNITS,HIDDEN_UNITS],stddev = 0.001))\n",
    "bias_gate = tf.Variable(tf.zeros([HIDDEN_UNITS]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OUTPUT LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs_weight = tf.Variable(tf.truncated_normal([HIDDEN_UNITS,NUM_FUT_PRED],stddev = 0.001))\n",
    "outputs_bias = tf.Variable(tf.zeros([NUM_FUT_PRED]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM INTERNAL LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTMCell(inp, prev_out, prev_hidden_cell_state):\n",
    "    i = tf.sigmoid(tf.matmul(inp,weights_input_gate) + tf.matmul(prev_out,weights_input_hidden) + bias_input)\n",
    "    f = tf.sigmoid(tf.matmul(inp,weights_forget_gate) + tf.matmul(prev_out,weights_forget_hidden) + bias_forget)\n",
    "    o = tf.sigmoid(tf.matmul(inp,weights_output_gate) + tf.matmul(prev_out,weights_output_hidden) + bias_output)\n",
    "    g = tf.tanh(tf.matmul(inp,weights_gate_gate) + tf.matmul(prev_out,weights_gate_hidden) + bias_gate)\n",
    "    \n",
    "    hidden_cell_state = f * prev_hidden_cell_state + i * g\n",
    "    out = o * tf.tanh(prev_hidden_cell_state)\n",
    "    \n",
    "    return out,hidden_cell_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'mul_1178:0' shape=(2, 256) dtype=float32>,\n",
       " <tf.Tensor 'add_3592:0' shape=(2, 256) dtype=float32>)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTMCell(tf.reshape(inputs[0][0],(-1,1)), np.zeros([NUM_FUT_PRED,HIDDEN_UNITS],dtype = np.float32), np.zeros([FEATURES,HIDDEN_UNITS],dtype = np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LSTM_Network_Loop(input_placeholder):\n",
    "    outputs = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        #for each batch, set the states\n",
    "        batch_hidden_cell_state = np.zeros([FEATURES,HIDDEN_UNITS],dtype = np.float32)\n",
    "        batch_output = np.zeros([NUM_FUT_PRED,HIDDEN_UNITS],dtype = np.float32)\n",
    "        \n",
    "        for feature_num in range(WINDOW_SIZE):\n",
    "            batch_output, batch_hidden_cell_state = LSTMCell(tf.reshape(input_placeholder[i][feature_num],(-1,1)), batch_output, batch_hidden_cell_state)\n",
    "            \n",
    "        outputs.append(tf.matmul(batch_output,outputs_weight) + outputs_bias)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_output = LSTM_Network_Loop(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll Use Mean Squared Error Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_squared_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mloss_collection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'losses'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'weighted_sum_by_nonzero_weights'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Adds a Sum-of-Squares loss to the training procedure.\n",
       "\n",
       "`weights` acts as a coefficient for the loss. If a scalar is provided, then\n",
       "the loss is simply scaled by the given value. If `weights` is a tensor of size\n",
       "[batch_size], then the total loss for each sample of the batch is rescaled\n",
       "by the corresponding element in the `weights` vector. If the shape of\n",
       "`weights` matches the shape of `predictions`, then the loss of each\n",
       "measurable element of `predictions` is scaled by the corresponding value of\n",
       "`weights`.\n",
       "\n",
       "Args:\n",
       "  labels: The ground truth output tensor, same dimensions as 'predictions'.\n",
       "  predictions: The predicted outputs.\n",
       "  weights: Optional `Tensor` whose rank is either 0, or the same rank as\n",
       "    `labels`, and must be broadcastable to `labels` (i.e., all dimensions must\n",
       "    be either `1`, or the same as the corresponding `losses` dimension).\n",
       "  scope: The scope for the operations performed in computing the loss.\n",
       "  loss_collection: collection to which the loss will be added.\n",
       "  reduction: Type of reduction to apply to loss.\n",
       "\n",
       "Returns:\n",
       "  Weighted loss float `Tensor`. If `reduction` is `NONE`, this has the same\n",
       "  shape as `labels`; otherwise, it is scalar.\n",
       "\n",
       "Raises:\n",
       "  ValueError: If the shape of `predictions` doesn't match that of `labels` or\n",
       "    if the shape of `weights` is invalid.  Also if `labels` or `predictions`\n",
       "    is None.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tf1.4/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.losses.mean_squared_error?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(target_placeholder,LSTM_output):\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(len(LSTM_output)):\n",
    "        losses.append(tf.losses.mean_squared_error(tf.reshape(target_placeholder[i],(-1,1)),tf.reshape(tf.diag_part(LSTM_output[i]),(-1,1))))\n",
    "    \n",
    "    loss = tf.reduce_mean(losses)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Mean_3:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'add_3199:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3263:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3327:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3391:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3455:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3519:0' shape=(2, 2) dtype=float32>,\n",
       " <tf.Tensor 'add_3583:0' shape=(2, 2) dtype=float32>]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LSTM_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Placeholder_10:0' shape=(7, 2) dtype=float32>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiag_part\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Returns the diagonal part of the tensor.\n",
       "\n",
       "This operation returns a tensor with the `diagonal` part\n",
       "of the `input`. The `diagonal` part is computed as follows:\n",
       "\n",
       "Assume `input` has dimensions `[D1,..., Dk, D1,..., Dk]`, then the output is a\n",
       "tensor of rank `k` with dimensions `[D1,..., Dk]` where:\n",
       "\n",
       "`diagonal[i1,..., ik] = input[i1, ..., ik, i1,..., ik]`.\n",
       "\n",
       "For example:\n",
       "\n",
       "```\n",
       "# 'input' is [[1, 0, 0, 0]\n",
       "              [0, 2, 0, 0]\n",
       "              [0, 0, 3, 0]\n",
       "              [0, 0, 0, 4]]\n",
       "\n",
       "tf.diag_part(input) ==> [1, 2, 3, 4]\n",
       "```\n",
       "\n",
       "Args:\n",
       "  input: A `Tensor`. Must be one of the following types: `float32`, `float64`, `int32`, `int64`, `complex64`, `complex128`.\n",
       "    Rank k tensor where k is 2, 4, or 6.\n",
       "  name: A name for the operation (optional).\n",
       "\n",
       "Returns:\n",
       "  A `Tensor`. Has the same type as `input`. The extracted diagonal.\n",
       "\u001b[0;31mFile:\u001b[0m      ~/anaconda3/envs/tf1.4/lib/python3.6/site-packages/tensorflow/python/ops/gen_array_ops.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tf.diag_part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X_train,y_train, EPOCHS ,BATCH_SIZE,WINDOW_SIZE,FEATURES,NUM_FUT_PRED, sitrep_at_epoch = 0.2): \n",
    "    '''X_train should be of shape (N, WINDOW_SIZE, FEATURES)'''\n",
    "    \n",
    "    inputs = tf.placeholder(tf.float32,(BATCH_SIZE,WINDOW_SIZE,FEATURES))\n",
    "    targets = tf.placeholder(tf.float32,(BATCH_SIZE,NUM_FUT_PRED))\n",
    "    \n",
    "    LSTM_outputs = LSTM_Network_Loop(inputs)\n",
    "    \n",
    "    MSE_loss = loss(targets,LSTM_outputs)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer.minimize(MSE_loss)\n",
    "    \n",
    "    session = tf.Session()\n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    loss = []\n",
    "    train_out = []\n",
    "    \n",
    "    for i in range(EPOCHS):\n",
    "        trained_scores = []\n",
    "        j = 0\n",
    "        while (j+BATCH_SIZE) <=len(X_train):\n",
    "            X_batch = X_train[j:j+BATCH_SIZE]\n",
    "            y_batch = y_train[j:j+BATCH_SIZE]\n",
    "            \n",
    "            out, epoch_loss, _ = session.run([LSTM_outputs, MSE_loss, optimizer], feed_dict={inputs:X_batch, targets:y_batch})\n",
    "            \n",
    "            train_out.append(out)\n",
    "            loss.append(epoch_loss)\n",
    "            j+=BATCH_SIZE\n",
    "        \n",
    "        if (i/EPOCHS) == sitrep_at_epoch:\n",
    "            print(\"At EPOCH: {}/{}, loss : {}\".format(i,EPOCHS,loss))\n",
    "            \n",
    "    return session\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf1.4",
   "language": "python",
   "name": "tf1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
